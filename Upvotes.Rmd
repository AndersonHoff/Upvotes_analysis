---
title: "UpVotes Analysis and Prediction" 
author: "Author: Anderson Hoff"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`" 
output:
  html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
    fig_caption: yes
    code_folding: show
    number_sections: true

fontsize: 14pt

---

<!---
- Compile from command-line
Rscript -e "rmarkdown::render('Upvotes.Rmd', c('html_document'), clean=FALSE)"
-->

The main objective of this project is to learn and practice some technical duties about data analysis and graph generation. Also, I tested different predictive models, focused on the behavior and differences between them. Last, but not least, it is an excellent oportunity to practice my English skills. 

# Overview
 
The objective of this problem is to identify the best question authors on an online question and answer platform. This identification will bring more insight into increasing the user engagement. Given the tag of the question, number of views received, number of answers, username and reputation of the question author, the problem requires you to predict the upvote count that the question will receive.
This test is provided by [Analytics Vidhya](https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/).

```{r library, eval=TRUE, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr) # for pipe
library(caTools) # for data partition
library(ModelMetrics) # for rmse
library(corrplot) # correlation matrix
```

# Data analysis

## Load data

The data used in this analysis is composed by a training set and a test set. In will be dealing with the training data. 

```{r LoadData, eval=TRUE, echo=FALSE}
train_data = read.csv('train_data.csv')

train_data <- train_data %>%
  select(Upvotes, everything()) 
```

```{r kable}
library(knitr)
kable(train_data[1:10,])
```

| Variable  | Definition |
|:---------:|:----------:|
Upvotes | Number of upvotes for the question (TARGET) 
ID | Question ID
Tag | Anonymised tags representing question category
Reputation | Reputation score of author
Answer | Number of times question has been answered
Username | Anonymised user id of question author
Views | Number of times question has been viewed

```{r, eval=TRUE, echo=TRUE}
sum(is.na(train_data))

glimpse(train_data)
```

## Graphics of data

```{r}
levels(train_data$Tag)
ggplot(train_data, aes(x=Tag)) +
  geom_bar()
```

```{r}
summary(train_data$Reputation)
reputation_groups <- cut(train_data$Reputation, breaks = 10, labels = FALSE)
table(reputation_groups)
```
```{r, echo=FALSE}
#hist(reputation_groups)
#train_data$Reputation <- sqrt(train_data$Reputation)
#hist(train_data$Reputation)
```

```{r, eval=TRUE}
summary(train_data$Answers)
ggplot(train_data, aes(x=Answers)) +
  geom_bar()

summary(train_data$Username)
Usernames <- cut(train_data$Username, breaks = 10, labels = FALSE)
table(Usernames)

ggplot(train_data, aes(x=Usernames)) +
  geom_bar()


summary(train_data$Views)
Views <- cut(train_data$Views, breaks = 10, labels = FALSE)
table(Views)
```

```{r, echo=FALSE}
#hist(Views)
#train_data$Views <- log10(train_data$Views)
#hist(logViews)
```


```{r, eval=TRUE}
summary(train_data$Upvotes)
Upvotes <- cut(train_data$Upvotes, breaks = 10, labels = FALSE)
table(Upvotes)

numeric.var <- sapply(train_data, is.numeric)
corr.matrix <- cor(train_data[,numeric.var])
corrplot(corr.matrix, main="\n\nCorrelation Plot for Numerical Variables", 
         method="circle")
```

In statistics, the correlation coefficient *cc* measures the strength and direction of a linear relationship between two variables on a scatterplot. The value of *cc* is always between +1 and –1. To interpret its value, see which of the following values your correlation *cc* is closest to: 

* -1 : indicates a strong negative correlation (if one variable increases, the other decreases).  
* 0 : there is no association between the two variables.  
* +1 : indicates a stron positive correlation: (both variables vary in the same way).  


```{r}
train_data2 <- train_data %>%
  select(-ID)

train_data <- train_data2
```

# Predictive Models

We start here the data forecast for this problem, Only to emphasize, the evaluation metric for this competition is RMSE (root mean squared error).  
The RMSE is a standard way to measure the error of a model in predicting quantitative data. Formally it is defined as 
$$RMSE = \sqrt{\sum(ŷ_i - y_i)/n}$$
The RMSE can be tought as a distance between the vector of predicted values and the vector of observed values.

## Linear Regression Model 

```{r, eval=TRUE}

set.seed(seed = 2019)
split = sample.split(train_data, SplitRatio = 0.7)#SplitRatio indicates the size of the training set
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)

lmmodel <- lm(Upvotes ~ +Tag +Reputation +Answers +Username +Views, 
              data = train_set, singular.ok = TRUE)
anova(lmmodel)
summary(lmmodel)

plot(lmmodel, which=1, caption = list("Residuals vs Fitted"))
```

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. 

```{r, eval=TRUE}
plot(lmmodel, which=2, caption = list("", "Normal Q-Q"))
```

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good if residuals are lined well on the straight dashed line.

```{r}
plot(lmmodel, which=3, caption = list("", "", "Scale-Location"))

```

It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.

```{r}
plot(lmmodel, which=5, caption = list("", "", "", "", "Residuals vs Leverage"))
```

This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don’t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don’t get along with the trend in the majority of the cases.

Source of graphic explanation: [University of Virginia](https://data.library.virginia.edu/diagnostic-plots/)

```{r, eval=TRUE}
test_set$predict <- predict(lmmodel, newdata =  test_set, 
                                        type = "response",
                                        interval = "confidence",
                                        family = poisson(link = ))

#test_set <- test_set %>%
#  select(predict, everything())

rmse_lm <- rmse(test_set$Upvotes, test_set$predict)
```

The RMSE for the testing sample is `r rmse_lm`.

To generate the results for the Testing sample, just run the code below, which generates a .csv file with the respective results. 

```{r, eval=FALSE}
test_data = read.csv('test_data.csv')
test_data$predict <- predict.lm(lmmodel, newdata = test_data, 
                                type = "response",
                                interval = "confidence",
                                family = poisson())

test_data$predict <- as.integer(test_data$predict)

write.table(cbind(test_data$ID,test_data$predict),
            file="Upvotes_LM.csv", 
            sep = ",", quote = FALSE, col.names = c('ID','Upvotes'), 
            row.names=F)
```



## Non-Linear Regresion Model

```{r, eval=TRUE}
train_data <- train_data2
set.seed(seed = 2020)
split = sample.split(train_data, SplitRatio = 0.7)
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)

glmmodel <- glm(Upvotes ~ ., 
                train_set, family = quasi(link = "identity"))
#anova(glmmodel)
summary(glmmodel)
#plot(glmmodel)

test_set$predict <- predict.glm(glmmodel, newdata =  test_set,
                                type = "response",
                                family = poisson())

nlm_rmse <- rmse(test_set$Upvotes, test_set$predict)
#3624  3108
```

The RMSE for the testing sample is `r nlm_rmse`.

## Neural net prediction

```{r, eval=FALSE}
library(neuralnet)
train_data <- train_data2
train_net <- train_data %>%
  select(-Tag, - Upvotes)

Upvotes <- select(train_data, 1)

norm.fun = function(x){ 
  (x - min(x))/(max(x) - min(x)) 
}

data.norm = apply(train_net, 2, norm.fun)

data <- as.data.frame(data.norm)

train_net <- cbind(Upvotes, data)

set.seed(seed = 2019)
split = sample.split(train_net, SplitRatio = 0.1)#SplitRatio indicates the size of the training set
train_set = subset(train_net, split == TRUE)
test_set = subset(train_net, split == FALSE)


nnmodel <- neuralnet(train_net$Upvotes ~ +Reputation +Answers +Username +Views, 
                     data = train_net,
                     linear.output = FALSE,
                     hidden = 2,
                     threshold=0.01,
                     lifesign = "minimum",
                     rep = 3)
#print(nnmodel)
#plot(nnmodel)
```

## XGBoost analysis

```{r, eval=FALSE}
library(xgboost)
library(Matrix)
library(Ckmeans.1d.dp)

train_data <- train_data2
set.seed(seed = 2019)
split = sample.split(train_data, SplitRatio = 0.7)
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)

traindumb <- sparse.model.matrix(Upvotes ~. -1 , data = train_set)

train_label <- train_set[,"Upvotes"]

mtrain_set <- xgb.DMatrix(data = as.matrix(traindumb), label = train_label)
#print(mtrain_set, verbose = TRUE)

testdumb <- sparse.model.matrix(Upvotes ~. -1, data = test_set)

test_label <- test_set[,"Upvotes"]

mtest_set <- xgb.DMatrix(data = as.matrix(testdumb), label = test_label)

xgb_param <- list("booster" = "gblinear", 
                  "lambda L2" = 1,
                  "objective" = "reg:logistic", 
                  "eval_metric" = "rmsle")

watchlist <- list(train = mtrain_set, test = mtest_set)

xgbmodel <- xgb.train(parameters = xgb_param, 
                      data = mtrain_set, 
                      max.depth = 10,
                      nrounds = 40,
                      watchlist = watchlist,
                      eta = 0.1)

eval <- data.frame(xgbmodel$evaluation_log)
plot(eval$iter, eval$test_rmse, col = 'blue')

xgbimpor<- xgb.importance(model = xgbmodel)
xgb.ggplot.importance(xgbimpor)

pred <- predict(xgbmodel, newdata = mtest_set)

result <- as.data.frame(cbind(test_set$Upvotes, pred))

rmse(test_set$Upvotes, pred)
#1256

ggplot(result,aes(result$pred, result$V1)) + geom_point(color = "darkred", alpha = 0.5) + 
  geom_smooth(method=lm)+ ggtitle('Linear Regression ') + ggtitle("Extreme Gradient Boosting: Prediction vs Test Data") +
  xlab("Predecited Upvotes") + ylab("Observed Upvotes") + 
  theme(plot.title = element_text(color="darkgreen",size=16,hjust = 0.5),
        axis.text.y = element_text(size=12), axis.text.x = element_text(size=12,hjust=.5),
        axis.title.x = element_text(size=14), axis.title.y = element_text(size=14))
```

# Session Info

```{r sessionInfo}
sessionInfo()
```