---
title: "UpVotes Challenge: Analysis and Prediction" 
author: "Author: Anderson Hoff"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`" 
output:
  html_document:
    toc: FALSE
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 2
    fig_caption: yes
    number_sections: False
fontsize: 14pt
---

<!-- Compile from command-line
Rscript -e "rmarkdown::render('Upvotes.Rmd', c('html_document'), clean=FALSE)"
-->

<!-- To comment a line Ctrl+Shift+C -->

#  {.tabset .tabset-fade .tabset-pills}

## OVERVIEW
<br>
The main objective of this project is to learn and practice some technical duties about data analysis and graphic generation. Also, I tested different predictive models, focused on the behavior and differences between them. Last, but not least, that is an excellent oportunity to practice my English skills.  

The challenge of this problem is to identify the best question authors from an on-line question and answer platform. This identification will bring more insight into increasing the user engagement. By this way, given the tag of the question, number of views received, number of answers, username and reputation of the question author, the problem requires you to predict the up votes number that determined question will receives.  

This test is provided by [Analytics Vidhya](https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/).  

## REQUIRED PACKAGES

```{r library, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
#if(!require(dplyr)) {istall.packages("dplyr")} else {library(dplyr)}
library(dplyr)
library(ggplot2)
library(magrittr) # for pipe
library(caTools) # for data partition
library(ModelMetrics) # for rmse
library(corrplot) # correlation matrix
library(knitr) # for floating tables 
library(neuralnet)
library(xgboost)
library(Matrix)
library(Ckmeans.1d.dp)
library(keras)
```

## DATA

The data used in this analysis is composed by a training set and a test set. In this part I will be dealing with the training dataset. This is how it looks like.

```{r LoadData, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
train_data = read.csv('train_data.csv')

train_data <- train_data %>%
  select(Upvotes, everything()) 

kable(train_data[1:10,], align="cr")

```

| Variable  | Definition |
|:---------:|:----------:|
Upvotes | Number of upvotes for the question (TARGET) 
ID | Question ID
Tag | Anonymised tags representing question category
Reputation | Reputation score of author
Answer | Number of times question has been answered
Username | Anonymised user id of question author
Views | Number of times question has been viewed

As can be seen, the target is to predict the number of positive votes a question receive, based on the features described above.  
The table below show the type of each column of the data. Apart from "TAG", all other columns are numeric.

```{r, eval=TRUE, echo=FALSE}
glimpse(train_data)

#summary(train_data)

```

It is important to identify missing values (NA's) on the tables, from both numeric ans categorical data. The result obtained below tell us that there are not empty cells in the numeric columns.

```{r, eval=TRUE, echo=TRUE}

sum(is.na(train_data))

levels(train_data$Tag)
```

Also, there is not a blank categorical cell in the TAG column, since the line above contain only letters.  
These results means the dataset is usable and we can start try to extract relevant information from it.

## GRAPHS {.tabset .tabset-fade .tabset-pills}

In order to observe how the data is spread in each column, some graphs  and tables were created. The ID column contain a different number to identify each question, and this number is never repeated. By this way, we will not use this column in the analysis. The results are separated for each column in the tabset below. 

### Tag 

The table below show us the question category is spread in different letters, and the graph indicates that it is reasobly distributed between them.  

```{r}
table(train_data$Tag)
```
<br>

```{r}
ggplot(train_data, aes(x=Tag)) +
  geom_bar(aes(fill=Tag)) + 
  ggtitle("TAG")+
  coord_flip()+
  theme_bw()+
  ylab("Frequency")+
  xlab("TAG")
```

### Reputation
```{r}
summary(train_data$Reputation)
reputation_groups <- cut(train_data$Reputation, breaks = 10, labels = FALSE)
table(reputation_groups)
```
<br>
From the table above, we observe that almost all of Reputation data is in the first column. This means most of the Reputation is low, and in this case, a graphic will not add any knowledge.  

### Answers

```{r, eval=TRUE}
summary(train_data$Answers)

ggplot(train_data, aes(x=Answers)) +
  geom_bar(fill = "blue") + 
  ggtitle("Answers")+
  coord_flip()+
  theme_bw()+
  ylab("Frequency")+
  xlab("Answers")
```

From the diagram above, most of the questions have a small number of answers. One question that arises is if there is a correlation between the number of answers and the upvotes. 

### Usernames

```{r, eval=TRUE}
summary(train_data$Username)

Usernames <- cut(train_data$Username, breaks = 10, labels = FALSE)
table(Usernames)
hist(Usernames)

#ggplot(train_data, aes(x=Username)) +
#  geom_bar(aes(fill = Username)) + 
#  ggtitle("Usernames frequency")+
#  coord_flip()+
#  ylab("Number of frequency")+
#  xlab("Usernames")
```

### Views
```{r}
summary(train_data$Views)
Views <- cut(train_data$Views, breaks = 10, labels = FALSE)
table(Views)
hist(Views)
```

```{r, echo=FALSE}
#hist(Views)
#train_data$Views <- log10(train_data$Views)
#hist(logViews)
```

### Upvotes

```{r, eval=TRUE}
summary(train_data$Upvotes)
Upvotes <- cut(train_data$Upvotes, breaks = 10, labels = FALSE)
table(Upvotes)
hist(Upvotes)
```

### Corr. Matrix

In statistics, the correlation coefficient *cc* measures the strength and direction of a linear relationship between two variables on a scatterplot. The value of *cc* is always between +1 and –1. To interpret its value, see which of the following values your correlation *cc* is closest to: 

* -1 : indicates a strong negative correlation (if one variable increases, the other decreases).  
* 0 : there is no association between the two variables.  
* +1 : indicates a stron positive correlation: (both variables vary in the same way).

```{r}
numeric.var <- sapply(train_data, is.numeric)
corr.matrix <- cor(train_data[,numeric.var])
corrplot(corr.matrix, order = "original", type="upper", diag = TRUE, tl.pos = "n", cl.pos = "n")
corrplot(corr.matrix, add = TRUE, type = "lower", method = "number", 
         order = "original", col = "black", diag = TRUE, tl.pos = "d", tl.cex = 0.77, cl.pos = "n")

```

  
<br>
From the results, we observe there is no correlation between Upvotes (the target) and ID or Username. Since there is one ID for each question, this result is reasonable. The same occurs for Username.  
Due to these results, we skip these two columns for the training process.

```{r}
train_data <- train_data %>%
  select( -Username)
```

## PREDICTIVE MODELS {.tabset .tabset-fade .tabset-pills}

We start here the data forecast for this problem, Only to emphasize, the evaluation metric for this competition is RMSE (root mean squared error).  
The RMSE is a standard way to measure the error of a model in predicting quantitative data. Formally it is defined as 
$$RMSE = \sqrt{\sum(ŷ_i - y_i)/n}$$
The RMSE can be tought as a distance between the vector of predicted values and the vector of observed values.

### Linear Regression Model  

```{r, eval=TRUE, warning=FALSE, message=FALSE}

set.seed(seed = 2019)
split = sample.split(train_data, SplitRatio = 0.7)
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)

lmmodel <- lm(Upvotes ~ +Tag +Reputation +Answers +Views, 
              data = train_set, singular.ok = TRUE)
anova(lmmodel)
summary(lmmodel)

plot(lmmodel, which=1, caption = list("Residuals vs Fitted"))
```

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. 

```{r, eval=TRUE}
plot(lmmodel, which=2, caption = list("", "Normal Q-Q"))
```

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good if residuals are lined well on the straight dashed line.

```{r}
plot(lmmodel, which=3, caption = list("", "", "Scale-Location"))

```

It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.

```{r}
plot(lmmodel, which=5, caption = list("", "", "", "", "Residuals vs Leverage"))
```

This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don’t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don’t get along with the trend in the majority of the cases.

Source of graphic explanation: [University of Virginia](https://data.library.virginia.edu/diagnostic-plots/)
<br>
The following step is to predict the values based on this model, and calculate the RMSE for the obtained results.  

```{r, eval=TRUE, warning=FALSE, message=FALSE}
predict_lm <- predict(lmmodel, newdata =  test_set, 
                                        type = "response",
                                        interval = "confidence",
                                        family = poisson(link = ))

predict <- as.data.frame(predict_lm)

test_set$predict <- predict$fit

ggplot()+
  geom_point(data=test_set, aes(x=ID, y=Upvotes), fill="red") +
  geom_point(data=test_set, aes(x=ID, y=predict), fill="blue", colour="darkblue")+
  theme_bw()+
  ylab("Predict, Upvotes")+
  xlab("ID")
  
rmse_lm <- as.integer(rmse(test_set$Upvotes, test_set$predict))
```

The RMSE for the testing sample is `r rmse_lm`.

<br>
To generate the results for the Testing sample, run the code below for the testing sample, after the data preparation, in the same way as the training sample. This will generate a .csv file with the respective results, which can be send to the site for automatic evaluation. 

```{r, eval=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
test_data = read.csv('test_data.csv')
test_data$predict <- predict.lm(lmmodel, newdata = test_data, 
                                type = "response",
                                interval = "confidence",
                                family = poisson())

test_data$predict <- as.integer(test_data$predict)

write.table(cbind(test_data$ID,test_data$predict),
            file="Upvotes_LM.csv", 
            sep = ",", quote = FALSE, col.names = c('ID','Upvotes'), 
            row.names=F)
```
<br>
This procedure is the same for any other techniques.

### Non-Linear Regresion Model  

```{r, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(seed = 2020)
split = sample.split(train_data, SplitRatio = 0.7)
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)

glmmodel <- glm(Upvotes ~ +Tag +Reputation +Answers +Views, 
                train_set, family = quasi(link = "identity"))

summary(glmmodel)
#plot(glmmodel)

test_set$predict <- predict.glm(glmmodel, newdata =  test_set,
                                type = "response",
                                family = poisson())

ggplot()+
  geom_point(data=test_set, aes(x=ID, y=Upvotes), fill="red") +
  geom_point(data=test_set, aes(x=ID, y=predict), fill="blue", colour="darkblue")+
  theme_bw()+
  ylab("Upvotes, Predicted")+
  xlab("ID")

rmse_nlm <- as.integer(rmse(test_set$Upvotes, test_set$predict))
#3624  3108
```

The RMSE for the testing sample is ` rmse_nlm`.

### Neural net prediction  

```{r, eval=FALSE, warning=FALSE, message=FALSE}
train_net <- train_data %>%
  select(-Tag, - Upvotes)

Upvotes <- select(train_data, 1)

norm.fun = function(x){ 
  (x - min(x))/(max(x) - min(x)) 
}

data.norm = apply(train_net, 2, norm.fun)

data <- as.data.frame(data.norm)

train_net <- cbind(Upvotes, data)

set.seed(seed = 2019)
split = sample.split(train_net, SplitRatio = 0.1)#SplitRatio indicates the size of the training set
train_set = subset(train_net, split == TRUE)
test_set = subset(train_net, split == FALSE)


nnmodel <- neuralnet(train_net$Upvotes ~ +Reputation +Answers +Username +Views, 
                     data = train_net,
                     linear.output = FALSE,
                     hidden = 2,
                     threshold=0.01,
                     lifesign = "minimum",
                     rep = 3)
#print(nnmodel)
#plot(nnmodel)
```

### XGBoost analysis  

```{r, eval=TRUE, warning=FALSE, message=FALSE}

number <- 2020
set.seed(seed = number)
split = sample.split(train_data, SplitRatio = 0.7)
train_set = subset(train_data, split == TRUE)
test_set = subset(train_data, split == FALSE)
```

```{r, eval=TRUE, warning=FALSE, message=FALSE}
traindumb <- sparse.model.matrix(Upvotes ~. -1 , data = train_set)

train_label <- train_set[,"Upvotes"]

mtrain_set <- xgb.DMatrix(data = as.matrix(traindumb), label = train_label)
#print(mtrain_set, verbose = TRUE)

testdumb <- sparse.model.matrix(Upvotes ~. -1, data = test_set)

test_label <- test_set[,"Upvotes"]

mtest_set <- xgb.DMatrix(data = as.matrix(testdumb), label = test_label)

xgb_param <- list("booster" = "gblinear", 
                  "lambda L2" = 1,
                  "objective" = "reg:logistic", 
                  "eval_metric" = "rmsle")

watchlist <- list(train = mtrain_set, test = mtest_set)
```

```{r, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
xgbmodel <- xgb.train(parameters = xgb_param, 
                      data = mtrain_set, 
                      max.depth = 10,
                      watchlist = watchlist,
                      nrounds = 40,
                      eta = 0.1)
```

```{r, eval=TRUE, warning=FALSE, message=FALSE}
eval <- data.frame(xgbmodel$evaluation_log)
plot(eval$iter, eval$test_rmse, col = 'blue')

xgbimpor<- xgb.importance(model = xgbmodel)
xgb.ggplot.importance(xgbimpor)

pred <- predict(xgbmodel, newdata = mtest_set)

result <- as.data.frame(cbind(test_set$Upvotes, pred))

rmse_xgb <- as.integer(rmse(test_set$Upvotes, pred))

ggplot(result,aes(result$pred, result$V1)) + geom_point(color = "darkred", alpha = 0.5) + 
  geom_smooth(method=lm)+ ggtitle('Linear Regression ') + ggtitle("Extreme Gradient Boosting: Prediction vs Test Data") +
  xlab("Predecited Upvotes") + ylab("Observed Upvotes") + 
  theme(plot.title = element_text(color="darkgreen",size=16,hjust = 0.5),
        axis.text.y = element_text(size=12), axis.text.x = element_text(size=12,hjust=.5),
       axis.title.x = element_text(size=14), axis.title.y = element_text(size=14))

rm(split, test_label,test_set, train_label, train_set, testdumb, traindumb,
   watchlist,xgbmodel, xgb_param, result, eval, mtest_set, mtrain_set)
```

For the XGBoost analysis, the RMSE for the testing sample is `r rmse_xgb`.

### Keras analysis  

```{r, eval=TRUE, message=FALSE, warning=FALSE}

train_data = read.csv('train_data.csv')

train_data <- train_data %>%
  select(Upvotes, everything())

train_data %<>% mutate_if(is.factor, as.numeric)

#train_data <- train_data %>% filter(train_data$Views < 100000)
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}

maxmin_data <- as.data.frame(lapply(train_data, normalize))

maxmin_data<-as.matrix(maxmin_data)

ind <- sample(2, nrow(maxmin_data), replace = T, prob = c(.7, .3))
train_set <- maxmin_data[ind==1, 2:7]
test_set <- maxmin_data[ind==2, 2:7]

train_target <- maxmin_data[ind==1, 1]
test_target <- maxmin_data[ind==2, 1]
```

```{r, eval=TRUE, message=FALSE, warning=FALSE}
k_model <- keras_model_sequential() 
k_model %>% 
  layer_dense(units = 18, activation = 'relu', 
              kernel_initializer='RandomNormal', 
              input_shape = c(6)) %>% 
  layer_dense(units = 12, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'linear')

#summary(k_model)

k_model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam',
  metrics = c('mae')
)

history <- k_model %>% fit(
  train_set, train_target, 
  epochs = 2, batch_size = 50, 
  validation_split = 0.1
)
#epoch = 7 - ~1400 epoch = 2 - ~2100

k_model %>% evaluate(test_set, test_target)

pred <- data.frame(y = predict(k_model, as.matrix(test_set)))
predicted=pred$y * abs(diff(range(train_data$Upvotes))) + min(train_data$Upvotes)
actual=test_target * abs(diff(range(train_data$Upvotes))) + min(train_data$Upvotes)
result <- data.frame(predicted,actual)

rmse_keras <- as.integer(rmse(result$actual, result$predicted))

```
<br>
The RMSE for this model is `rmse_keras`.

## SUMMARY RESULTS

The results of RMSE for each model is summarized below. There are some parameters to be fine tuned, which may decrease these values.  

| METHOD  | RMSE |
|:---------:|:----------:|
Linear Regresion | `r rmse_lm`  
Non-linear Regresion | `r rmse_nlm`  
XGBoost Analysis | `r rmse_xgb`
Keras Analysis |  `r rmse_keras`

